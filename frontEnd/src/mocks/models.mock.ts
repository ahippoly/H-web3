export const modelsMocks: ModelAI[] = [
  {
    name: 'VQ-VAE-2',
    description: 'VQ-VAE-2 is a neural network architecture for image generation and compression. It is based on the Vector Quantized Variational AutoEncoder (VQ-VAE) architecture, which uses a discrete latent space to represent images. This allows for high-quality image generation and compression, with the ability to control the level of detail in the generated images. VQ-VAE-2 improves on the original VQ-VAE architecture by using a hierarchical structure that allows for better modeling of complex image data. This makes it well-suited for a wide range of image generation and compression tasks, including image super-resolution, style transfer, and image synthesis.',
    linkUrl: '',
    summary: 'VQ-VAE-2 is a neural network architecture for image generation and compression.',
  },
  {
    name: 'BigGAN',
    description: 'BigGAN is a state-of-the-art generative adversarial network (GAN) architecture for image generation. It is based on the popular GAN architecture, which uses two neural networks – a generator and a discriminator – to learn to generate realistic images. BigGAN improves on the original GAN architecture by using a larger and deeper neural network, which allows it to generate higher-quality images with more detail and realism. This makes it well-suited for a wide range of image generation tasks, including image synthesis, style transfer, and image editing.',
    linkUrl: '',
    summary: 'BigGAN is a state-of-the-art generative adversarial network (GAN) architecture for image generation.',
  },
  {
    name: 'BERT',
    description: 'BERT (Bidirectional Encoder Representations from Transformers) is a state-of-the-art natural language processing (NLP) model developed by Google. It is based on the Transformer architecture, which uses self-attention mechanisms to model the relationships between words in a sentence. BERT improves on previous NLP models by using a bidirectional approach, which allows it to capture the context of a word based on both the words that come before and after it. This makes it well-suited for a wide range of NLP tasks, including text classification, named entity recognition, and question answering.',
    linkUrl: '',
    summary: 'BERT is a state-of-the-art natural language processing (NLP) model developed by Google.',
  },
  {
    name: 'GPT-3',
    description: 'GPT-3 (Generative Pre-trained Transformer 3) is a state-of-the-art natural language processing (NLP) model developed by OpenAI. It is based on the Transformer architecture, which uses self-attention mechanisms to model the relationships between words in a sentence. GPT-3 improves on previous NLP models by using a large and deep neural network, which allows it to generate human-like text with high accuracy and fluency. This makes it well-suited for a wide range of NLP tasks, including text generation, language translation, and dialogue systems.',
    linkUrl: '',
    summary: 'GPT-3 is a state-of-the-art natural language processing (NLP) model developed by OpenAI.',
  },
  {
    name: 'ResNet',
    description: 'ResNet (Residual Neural Network) is a state-of-the-art deep learning architecture for image classification. It is based on the concept of residual learning, which uses skip connections to allow the network to learn the residual mapping between layers. ResNet improves on previous deep learning architectures by using a deeper and wider neural network, which allows it to learn more complex features and achieve higher accuracy on image classification tasks. This makes it well-suited for a wide range of computer vision tasks, including object detection, image segmentation, and image recognition.',
    linkUrl: '',
    summary: 'ResNet is a state-of-the-art deep learning architecture for image classification.',
  }]
